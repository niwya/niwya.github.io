{"componentChunkName":"component---src-gatsby-theme-portfolio-minimal-templates-article-listing-index-tsx","path":"/projects","result":{"pageContext":{"articles":[{"banner":{"alt":"","caption":"","src":null},"body":"<!--**bold**\n    *italics*\n    ## headline\n    ### subheadline\n    #### subsubheadline -->\n<div style=\"text-align: justify\">\n<h2>Abstract</h2>\n<p>   Safely relying on Global Navigation Satellite Systems (GNSS) measurements for position estimation using multi-sensor navigation algorithms, especially in critical phases of flight - such as takeoff or landing - requires precise knowledge of the errors affecting position estimates and their extrema values at any time. This work investigates a method for characterization and power-spectral density (PSD) bounding of GNSS carrier phase multipath error intended for use in sensor fusion for aircraft navigation. In this dissertation, two methods of GNSS carrier phase multipath characterization are explored: single frequency dual antenna (DA) and single antenna dual frequency (DF). However, since not all aircraft are equipped with multiple GNSS antennas, because the DA method entails a meticulous tracking of the lever arm between the two antennas, and as multipath seen by two antennas in a short baseline configuration may cancel out, the DF method is preferred and is the main emphasis of this work. By subtracting carrier phase measurements collected by a receiver over two distinct frequencies, a composite measurement containing ionospheric delay and carrier phase multipath is obtained. The ionospheric delay has slower dynamics than multipath, so it removed using a high pass filter. The filter cutoff frequency is carefully picked based on a study of ionospheric delay dynamics. The DF method is validated on a rooftop GPS carrier phase dataset, and finally, directions and considerations for its ultimate intended use on airborne collected GNSS carrier phase data are provided.</p>\n<hr>\n<h2>Overview</h2>\n<p>   To help you understand the key challenges of my thesis, here's a little poster:</p>\n<object data=\"/MSThesis_poster.pdf\" type=\"application/pdf\" width=\"700px\" height=\"450px\">\n    <embed src=\"http://chloebenz.com/MSThesis_poster.pdf\">\n        <p>This browser does not support PDFs. Please download the PDF to view it: <a href=\"http://chloebenz.com/MSThesis_poster.pdf\" target = \"_blank\">Download PDF</a>.</p>\n    </embed>\n</object>\n<hr>\n<h2>Credits &#x26; Publications</h2>\n<p>   This Master's thesis was undertaken under the supervision of Pr. Boris Pervan at the <a href = \"http://www.navlab.iit.edu/\" target = \"_blank\"> Navigation Lab </a> at IIT in Chicago, IL, USA, and with the help of Elisa Gallon, at that time PhD student at IIT.</p>\n<p>   You can read my full thesis <a href = \"/MSThesis.pdf\" target = \"_blank\"> here </a>. At this time, no other related publication is either in the works or has been published.</p>\n</div>","categories":["Thesis","Research"],"date":"April 24, 2022","description":"Basic restaurant knowledge domain in CR-Prolog SPARC for an autonomous agent to plan diverse  customer-oriented tasks and collaborated on linking it to a simulated environment made with PyBullet.","id":"e33bb499-2560-5cae-a726-d11f6074d44e","keywords":["Carrier Phase","GNSS","Multipath","Ionospheric Delay"],"slug":"/projects/carrier-phase-multipath/","title":"Carrier Phase Multipath Characterization And Frequency Domain Bounding","readingTime":{"text":"2 min read"}},{"banner":{"alt":"Morbier cheese","caption":"","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='660'%20height='400'%20viewBox='0%200%20660%20400'%20preserveAspectRatio='none'%3e%3cpath%20d='M0%208l1%207%202%203c1%203%203%203%204-1%201-2%202-2%202-1%200%202%201%202%209%202l9%201h5l-2-1%201-1h5c1%202%202%202%202-1l-1-2h-2l1-3h3l5-2%203-2h1l6-1c5-3%2011-3%2013-1h3c3-3%205-4%208-4h3L41%200H0v8m130-6c0%201%2015%202%2015%200l1-1v2c-1%201-1%201%202%201l10-1%205%201%205-1%201-1%201%201c-1%200%203%203%205%203l1-3V1l2%202c1%203%202%203%202%201%202-4%203-4-25-4-23%200-26%200-25%202m199-1l6%201c0%201%201%202%203%201l6%201h8c1%201%202%201%201%202v1h12l3%201c3%200%204%200%204%202s0%202%202%202c1-1%202%200%203%201%201%202%201%202%204%201h3c1%201%201%201-1%201-3%200-3%201%201%203l2%202%201%201%201-3v-2l2%202%205%202%204%201%203%201%203%203c1%202%202%204%204%204%202%201%202%201%201%202a655%20655%200%200168%203%202063%202063%200%2001146%203c18%201%2023%202%2024%204%200%201-8%201-15-1h-28c-8-1-9-1-11%201s-3%202-22%202a126%20126%200%2001-26-1h-7l1%203c0%203-1%204-3%202v-3c1-2-2-3-6-2l-8%201c-4-1-7%200-7%201h-4c0-2-5-2-6-1h-2c-1-1-1-1-3%201h-2c-2-3-9-3-10-1-2%203-4%202-3-1l-4-1a7455%207455%200%2000-24%200c-1-1-8%201-8%203-1%201-4%200-4-1l3-1c4-1%206-3%205-4l-7%202c-4%201-8%202-19%202h-12l-2%201-2%202-1%201-2-1c0-3-4-4-18-4l-23-1c-3-1-5-1-7%201s-3%202-4%200h-4c-2%202-3%202-4%201h-6l-7-1-11%201-6%202c-2%202-6%201-6-1h-20c0-2-7-1-7%200%200%202-3%205-5%205h-2c-3%201-4%200-2-1%202-3-1-2-4%200-2%202-4%202-2%200s1-3-1-2h-2l-20-1-11%202h-3c-2-1-3-1-6%201h-12l-3%201-4%201c-2%202-4%201-2-1v-2l-13%203h-3l-1%201-1%201-2-1-11%201c-3%202-8%201-7%200l-2-1v2c1%201%201%201-1%201s-2%200-2%202c1%202%200%203-3%202l-1-1%202-2c0-2-1-2-3%200h-3c-2-2-12-1-11%201l-3%203c-3%201-4%203-1%205h8l1-1c2%201%204%200%203-2l1-1%203%202%202%201h3l5%201%2014%201a1719%201719%200%200178-1c2-2%208-2%2015%200%202%201%205%201%205-1%200-1%2021-1%2024%201l7%201%207%201h2c2-2%203-1%203%201-1%201-1%201%201%201h3l1%201%201-2%202-1h2l5%205c1%200%202%200%201-1l4-1h6c1-1%202%200%204%201%201%202%201%202%205%200s13-3%2013-1c1%203%207%205%2012%204h7c5%201%207%201%208-1%201-1%201-1%202%201h11l4%201-1%201-1%201%202%201%201-1%202-1%202%201%204-1h5c2%203%208%206%2013%206l7%201c1%201%201%201%201-1l1-2h1l1%201c1-1%201%200%201%201%200%202%201%203%203%201%201-2%202-1%202%202%200%202%200%202%205%202l4-1c0-2%201-2%204-1l3%203c-1%205%200%206%202%203l2-3c1-1%201-1%202%201h2c0-2%201-2%204-1%206%200%207%201%207%203l2%204%202%202-2%201h1c5%200%208%201%208%202l-2%201h-3v2h2c3%200%204%202%204%208%200%205%200%206%202%207l2%202c-1%201%200%202%201%203%204%203%202%205-4%205-4%200-5%200-4%201l5%201h5l1%203h4c2-2%203-8%201-8v-1l4-1v1l-2%203%202%203v1c-1%200-2%201-2%203l-2%204v2l1%205%202%204h3c2%200%200%202-2%202l-3%202-4%201c-3%200-4%200-4%203%200%202%201%202%205%202%203%200%204%200%204-2s0-2%205%200c3%201%204%204%202%204a142%20142%200%2000-11%201c-4%200-5%200-5%202l-4%201-5%201%202%201%203%201c0%202-2%205-3%203l-2-3c-1-1-1-1-1%202l-1%205c-3%200-7%203-7%205l-4%201c-3%200-4%200-4%202%200%201%203%202%203%200h1l2%201v1c-2%202-6%202-6%201l-9%201-4%201-3%203h-2l-1%201-2-1-4-1c-11%200-14%200-14-2v-1l-5-2c-6-3-9-3-9-1l-4%205-7%205c0%203%200%203-1%202l-2%201c0%202-9%201-13-1l-6-2-1-1-14-3c-2%200-3%201-3%203l-2%202-2%201-3-1c-6-2-9-3-9%200%201%202%201%202-1%202l-3%202-2%203-2%201-1%201-2-1-4-2c-4-2-4-3-4-6l-1-3-6-2c-3-1-5-2-6-1h-21l-12%202-8%201-5%201c-3%200-3%200-2%202%200%203-3%207-6%207-4%200-8%204-5%206%203%200%201%205-2%206a171%20171%200%2000-7%203l-7%203-7%202h-4c-2%201-5%202-8%201l-5%201%2021%202c12%200%2017-1%2024-4l13-3%209-1%202-1%203-2c3-3%2016-8%2027-8%207-1%2013%201%2014%205%200%204%201%205%206%208l6%204c3%203%206%202%2011-2%202-2%204-4%205-3l10-4c4-2%2010-2%2012%200h1l3%201c1%202%203%202%204%202h1c-2%202-2%202%201%202%205%200%207%201%207%203s4%200%204-2c1-2%202-2%205-2h4l3-1h4c1-1%203%200%205%201l5%202%204%202h6l7%201%209%202-1%201c-2%201-1%201%203%201s4%200%204%202%200%203%202%204c4%200%208%200%207-2%200-2%201-1%203%201%203%203%202%209-1%209l-2%201-3%201c-5-2-6%201-2%205l4%203%201%201%201%201%203%201c2%202%205%202%205%200l2-1%201%201c0%202%203%205%204%205l1-1c0-2%200-2%203-1%202%201%203%201%202%202h-1l-3%201c-1%201%200%201%204%201h6l-4%201h-4v4l1%204v1c-1%201-1%201%201%202%201%202%201%202-1%202s-5-3-4-4c0-2%200-2-3%200-3%201-4%206-1%209v14a330%20330%200%2001-3%205v1c-2%201-2%200-2-3v-3l-2%204c-1%204-2%205-4%206l-2%201-2%201c-3%200-7%203-7%204l4-1%203-1%203-1c0-2%203%202%203%204l-4%201-3%201-1%202-2%204-2%204-2%201h-3l-1%201-1%203-3%203c0%201-1%202-3%202-4%200-5%200-5%203-1%202-1%203-4%203-4%201-7%202-4%202l1%201c0%202-7%203-13%203-3%201-4%204%200%204l2%201h-1l-6%202c-3%202-5%202-3%200%201-2%201-2-1-2l-4%201c0%201-1%202-2%201l-1%201%201%201%201%202-5%201-6%201-4%201-4%201-2%202h-3l-2-1-2%202-2%202a518%20518%200%2001-16%202h-1c-3-3-11-3-13%200-2%202-2%202-3%201h-9l-2%201-13%202-3%201c-1%202-9%202-9%201h-2l-15%202-15%201h-19c-17%200-17%200-19%202-3%203-5%204-5%201h-18c-10%200-10%200-12%202l-2%205%201%205%203%205a2004%202004%200%200190%204l182%201h165V0H492L329%201M89%2060l-3%201c-3%200-4%208-1%2010%201%202%206%203%208%201%202-1%202-9%201-11-2-2-5-2-5-1M1%20120c0%2030-1%2028%2011%2024%208-3%2024-5%2024-3l2%201%204%203%202%201%201%201c0%201%207%205%2010%205%204%200%208-2%2011-5%204-4%2013-4%2029-3a726%20726%200%200030%202l-2-5c0-2-5-9-6-10-2%200-5-6-8-15-3-8-6-12-4-7%200%203%200%203-1%202l-2-3h-3l-3-1-3-2c0-2-6-5-6-3l-2%201c-5%200-7%200-8%202l-3%201c-3%200-5%204-4%206l1%201%201%203c1%205%200%205-7%206-14%200-17-1-19-5l-4-4-1-1c0-1-4-2-5-1h-3l-1%202-1%202-1%205c0%207-2%208-8%206-5-2-8-7-10-15l-2-4-1-3c1-1%200-2-1-2l-1-3c1-3%201-3-3-4l-3-1v26m197%2084l1%201%207%206c7%208%2018%2013%2018%208%200-2%201-3%202-3%203-2%202-2-7-7l-7-4c-4-2-13-3-14-1M1%20311v29l19%201a1333%201333%200%2000102%203%208383%208383%200%200092%201c0-1%200-2%202-2s3-1%200-1l-1-4c0-5-3-14-7-16-3-2-5-5-4-10v-5c-1-1-1-1-1%201%200%203-1%205-2%203h-2c-1%200-2%200-1-2%200-2-3-3-5-1h-8l-5-1c-2-1-15-2-21-1h-3c1-2-3-1-4%201-1%201-2%201-4-1h-13l-5-1-2-1h-3c-1-2-5-1-7%200-1%201-3%200-3-2l-4-1c-3%200-5%201-5%202l-9-1-5-1-4-1h-9c-2%202-10%200-10-3h-1l-4-2c-2-2-3-3-4-2h-7c-2-1-2-1-3%201h-2l-2-1c-1%201-4%201-5-1l-3-1-3-1h-3l-1-1-3-2-3-1h-4l-1-2h-2c0%201-1%202-2%201h-4c-2%200-3-1-3-2H2c-1-1-1%208-1%2028'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/da5e7f1db66ba6403cf06e6f860d7735/37a0c/morbier_0.jpg","srcSet":"/static/da5e7f1db66ba6403cf06e6f860d7735/a9008/morbier_0.jpg 150w,\n/static/da5e7f1db66ba6403cf06e6f860d7735/1c084/morbier_0.jpg 300w,\n/static/da5e7f1db66ba6403cf06e6f860d7735/37a0c/morbier_0.jpg 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/da5e7f1db66ba6403cf06e6f860d7735/40ac4/morbier_0.webp 150w,\n/static/da5e7f1db66ba6403cf06e6f860d7735/9e761/morbier_0.webp 300w,\n/static/da5e7f1db66ba6403cf06e6f860d7735/2ea1b/morbier_0.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":660,"height":400.40000000000003}}}},"body":"<!--**bold**\n    *italics*\n    ## headline\n    ### subheadline\n    #### subsubheadline -->\n<div style=\"text-align: justify\">\n<h2>The main idea</h2>\n<p>    As a French person, I love myself a good cheese board at a restaurant. And from time to time, one cheese out of the selection is just celestial. In that case, I want to know what cheese it is - and to keep its info, to treat myself sometime in the future. I could just ask the restaurant personnel - right? Or, develop my own app so that if I take a picture of a delicious piece of cheese, the app tells me exactly what it is - kind of when you want to know what a piece of music is and you use Shazam.</p>\n<p><em>Note: this was a school project conducted at IIT, but to be honest, I love the idea so much that I will probably improve this project over time and maybe develop an actual app in the future. I'll post any update here!</em></p>\n<hr>\n<h2>The dataset</h2>\n<p>   The dataset is a <u>custom-made</u> dataset from images scraped from Google Images.</p>\n<h3>Scraping the images from Google</h3>\n<p>    The images have been scraped from Google Image with the help of Python module <a href = \"https://github.com/hardikvasa/google-images-download\" target = \"_blank\">google_images_download</a> using queries whose keywords corresponded to the cheeses I wanted to include. In the state of things, the dataset comprises:</p>\n<ul>\n<li><u>Beaufort</u> · 403 images</li>\n<li>Mozzarella · 288 images</li>\n<li><u>Comté</u> · 441 images</li>\n<li>Morbier ·347 images</li>\n<li>Bleu d'Auvergne · 386 images</li>\n</ul>\n<p>   Unfortunately, at the time of this project, Google Images did not allow more than 400 images to be scraped at once - always taken from the top of the Google Images query, so multiple queries under the same keywords yielded identical images.\nNote that Beaufort and Comté are highlighted in the above list - because they're very similar looking cheeses (look it up - by just looking at them, it's hard to tell them apart). In fact, I chose those two because of that characteristic - I wanted to assess my classifier's performance on a dataset with all different cheeses and see if including a cheese that's similar to another changed the metrics.</p>\n<h3>Data augmentation</h3>\n<p>   A few hundreds of images per cheese seemed too few to train a classifier - so I used data augmentation techniques to inflate my dataset. The images were subjected to:</p>\n<ul>\n<li>Random flip</li>\n<li>Random rotations</li>\n<li>Random zoom</li>\n</ul>\n<p>Those operations were performed using TensorFlow Keras' <a href = \"https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing\" target=\"_blank\">preprocessing functions</a>.</p>\n<hr>\n<h2>Classifier architectures</h2>\n<p>   Multiple classifier architectures were used to train models and confront the results.</p>\n<h3>Base model</h3>\n<p>   The base model has the following architecture:</p>\n<div align=\"center\">\n<table>\n<thead>\n<tr>\n<th align=\"left\"><strong>Layer</strong></th>\n<th align=\"center\">    <strong>Output Shape</strong>    </th>\n<th align=\"right\"><strong>Activation</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">Rescaling</td>\n<td align=\"center\">256·256·3</td>\n<td align=\"right\">·</td>\n</tr>\n<tr>\n<td align=\"left\">Conv2D</td>\n<td align=\"center\">256·256·32</td>\n<td align=\"right\">ReLu</td>\n</tr>\n<tr>\n<td align=\"left\">MaxPooling</td>\n<td align=\"center\">128·128·32</td>\n<td align=\"right\">·</td>\n</tr>\n<tr>\n<td align=\"left\">Conv2D</td>\n<td align=\"center\">128·128·32</td>\n<td align=\"right\">ReLu</td>\n</tr>\n<tr>\n<td align=\"left\">MaxPooling</td>\n<td align=\"center\">64·64·32</td>\n<td align=\"right\">·</td>\n</tr>\n<tr>\n<td align=\"left\">Conv2D</td>\n<td align=\"center\">64·64·32</td>\n<td align=\"right\">ReLu</td>\n</tr>\n<tr>\n<td align=\"left\">MaxPooling</td>\n<td align=\"center\">32·32·32</td>\n<td align=\"right\">·</td>\n</tr>\n<tr>\n<td align=\"left\">Flatten</td>\n<td align=\"center\">32768</td>\n<td align=\"right\">·</td>\n</tr>\n<tr>\n<td align=\"left\">Dense</td>\n<td align=\"center\">128</td>\n<td align=\"right\">ReLu</td>\n</tr>\n<tr>\n<td align=\"left\">Dense</td>\n<td align=\"center\">5</td>\n<td align=\"right\">Softmax</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>   Schematically, all the layers (omitting the rescaling layer) are represented on the following sktech:\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c825883daadec3994af8c3d4b6d19c6b/bf6f8/cheezam-baseNetwork.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABRNAAAUTQGUyo0vAAAC6ElEQVQozz2SW2hUZxSF9yQxSmPBktbUYKyTTEzMzVxIHCWiDaUgFYTSFxGpVJoYZHKZM2HS0DiOk7mc+c/5z9n7PzPGVJNJQzVRo6QQBW01iibgpVAUEdvHgD4INn20MLskog+LBd/Ltx4W/Cc0D4vuT6dsATFyIE4OzFoJF+vtwHonQOY5DKMNT8wQcPhzyIoAsNAAbl0EjnWBMZEEuHEF2NDcb0TfOmAZ2Pq3+WObTVRkI+V/k/oZ5qwosOjeyMLfwknfxw/kSfjdirombeH6Q4ZzXhv9q9nXChz1Fc2nQ7s+mf3FywmtgWXXWvB3dZcMp1PbbNvejIgVDuJ6g9TWO9aQ+6k8vmfaTrZEKPVRBk3ggQ0uFj0NLHqqWPrWsN7bHB2XZV+dO7NvwRhq1h21Dto7j9WPjY+XSNMsIMRNiLhDIW5OkqqNUKpekHI7iGWI6LGIyu/Lk95RlI13zVgJ+2vy8jqmcqelrJ+3IofSaLshPhStSKecihiqVSapCkJstYhyCbHRQWwOUTrXIFVJiF5ErNRJ7R6R6B48TW3fTo7uwNNnPhPxhOdPK7xv2k6WQu/5TFPatBoUYfEoyhpE+gIR3fR20a4RtEonbLPKRtpOiFuQsC2Kw0VHUmlv5NrMd2NXp78+cW+u7Lk+UHsOzTqIy3ihIuV5JMPVS0awbhjt0iSpfJMo30H0vDL6y5eMYHmSVLGNtHYEreLHMtTyQg56Z4PBwsu3f93p3J87sBjr6xxTdh3MnNKBuwBY9Nay0BpZ/77gmTwOT2UIOLI3j4XWxELbwkP7c1dY/OAqFr3LrIV/+DK//tLN1aZS2x7JcNMCRgEu/4TQrCaB9aPASR9ct2I5kPkLfrNisD19YVkE/xrBnIcynDPh6MAB9wpj0QPziUFQUn74xAy1suH/4KU5AMCZCLDZB1mhvQ+/S+LwSi+fOWtokD17ELKnOiBr+l1ZEXBx+AhsXPwH2AhUs9CKlyX/A5oye6iCuGraAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"&quot;Base model architecture&quot;\"\n        title=\"Figure - Base network architecture\"\n        src=\"/static/c825883daadec3994af8c3d4b6d19c6b/c1b63/cheezam-baseNetwork.png\"\n        srcset=\"/static/c825883daadec3994af8c3d4b6d19c6b/5a46d/cheezam-baseNetwork.png 300w,\n/static/c825883daadec3994af8c3d4b6d19c6b/0a47e/cheezam-baseNetwork.png 600w,\n/static/c825883daadec3994af8c3d4b6d19c6b/c1b63/cheezam-baseNetwork.png 1200w,\n/static/c825883daadec3994af8c3d4b6d19c6b/d61c2/cheezam-baseNetwork.png 1800w,\n/static/c825883daadec3994af8c3d4b6d19c6b/bf6f8/cheezam-baseNetwork.png 2047w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>   The rescaling layer is only there because the initial images are RGB images with different sizes - but they all need to be the same shape (256·256·3) once fed as input to the classifier.</p>\n<h3>Base model with dropout</h3>\n<p>   The base model is used, but with a few tweaks. Dropout layers are added after the second and third convolutional layers. These dropout layers randomly set inputs to 0 <u>with a frequency of 0.4</u> (during training only). This helps avoiding the overfitting of our model - <em>i.e.</em> it prevents the model from picking up too many details from the training data that may not be generalizable.</p>\n<h3>Base model with dropout and data augmentation</h3>\n<p>   Here once again, the base model is incremented with three new layers: <u>RandomFlip</u>, <u>RandomRotation</u> and <u>RandomZoom</u> , with parameters <u>'horizontal'</u>, <u>0.1</u> and <u>0.1</u> respectfully. Those layers are placed right after the resizing layer, and before the first convolution. This layer will provide artificial new images, by messing a bit with the images from the initial dataset. This way, the network is more robust to changes in size or in orientation of the cheeses. The operations performed can be visualized on the figure below - the rotate and zoom operations have been exaggerated for convenience:\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2e58eef92e540ec9609ea46850cef878/71c1d/cheezam-dataAugmentation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.33333333333334%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAABRNAAAUTQGUyo0vAAAEDUlEQVQ4y41Ua0zbVRQ/XWkplIcTaHmUtoQQRGEzIW6JWbJPakyWLNFFsmTMLH5QEV3be0uLaQDHY/333tKO12SACyvQDlwwYniULdsUzTCZzmiyJTqNAdTpjNMopEDvMbeAivODJzm5N/ec87vnDfAvEoxssVYwki8YMQlOCwWjJsGIRTCSsaXzv0gwoto0MCEjOb91NSThiZc16LUlI6fGdU6t2FwDdwMuFUQjALMRSJz3kXyMngdkRIWKHdY5tay11+mxtbYgFnA/fK+nKWVZsSehYjcpgxxWOd0A/CdvB9vgswMnVR/2NAL6HBb0HEv9pafp6O1Q4PivnR4jOqo0759uKoNrUzsBcctmO/Bfl9nzG+fncyCVP+luKEfXEe3ViUH9FyMdKXfPtAByp+5OwG2FG1dSNTPhSohG0u4DTeRg9u8fMqaHs2FuomC+u+EhVOw5eKhSha4jmu86PclrnGYjI5amUDtkTQ1lQjSyB6KRjO0ezm7LQ9qOaMSwb7wf0FWtRp/DtBSsL5nqPfEottSa0ecoQUYy8UA5wOULsAlm/e+QoxEjRCNPwMVRFcxPAwLAte5GgPjvapibMACuqWUqsO1VwKfLAC6NbYVpgGgkCS6OQur0CECovw0AY5A0E67UzYQtz4ydVqNig46zPpUE2Dk1bNbNhPenzIzsTZ8e3rV/vE8DizehS8pvfSxBjHmToVT44Ssofm8QoOWcP1EE+ODd3JpwB6z4nbLCsn0AvTYY7WsFuPqOCX78Oun7oBtkDyZ03EeharQH4Mp4Pnw0mcEGGcD1SwDYWqv99lR9Ud9b3sdQsVti3FmEjBQJRnJWOc1HxW6O9LdVLATri9FnLxac5AlGsuJS5nPk3Or0mO8E6wuRkdJlv3MHxDktlILnJocg7/plQE41gpE0wcjjgjsfQPshNdLDOuRUHeNUjt0jmyNZsdZe9xT6nWXY9KJBMFIiGNHJ0ArXOU3/ubuxDH32Pei16WRYgtNKbK1Nu9fTWLoSdB9Ed7V2MRRQISNWwWkxMlKHnmPl2PZa9XLH61xwWoo+R7L00IxvvJR5OxR4cmnA68GTxw0LA175XoEtrxgWzvp2fzkUtK22u3Z9NvYmxLkz0T5xv9N14+3e1pH56PM3w10EFdvuWMCdLD2xos+R/keHBz69cCZVzjEqdrkgyuJ+p/Wn3mZYGvBqsWovYMMLqjinBajY8mMB94FvzvldiwPefdhcIz/JjXOqh8RK4jQ/zp0p2Fyjx438ZUtAZKQkFnDrUbGlo2LXC06NMkWC0yJU7NloezYLaw/qVk7VpwhOy5ERrQSURchN7DxOLZs7T1ZZLxh5UGwAmAWTTIoFo5nSUHBqWg248uLcubkviTHOnfAnSo5b42UHgjkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"&quot;Data augmentation operations&quot;\"\n        title=\"Figure - Data augmentation operations\"\n        src=\"/static/2e58eef92e540ec9609ea46850cef878/c1b63/cheezam-dataAugmentation.png\"\n        srcset=\"/static/2e58eef92e540ec9609ea46850cef878/5a46d/cheezam-dataAugmentation.png 300w,\n/static/2e58eef92e540ec9609ea46850cef878/0a47e/cheezam-dataAugmentation.png 600w,\n/static/2e58eef92e540ec9609ea46850cef878/c1b63/cheezam-dataAugmentation.png 1200w,\n/static/2e58eef92e540ec9609ea46850cef878/71c1d/cheezam-dataAugmentation.png 1536w\"\n        sizes=\"(max-width: 1200px) 100vw, 1200px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3>VGG-16 with dropout and data augmentation</h3>\n<p>   VGG-16 - with VGG standing for <em>Visual Geometry Group</em> - is a 16 layers deep convolutional neural network architecture. It was developed by K. Simonyan and A. Zisserman from the eponymous Visual Geometry Group of the University of Oxford, UK. This architecture, along with the VGG-19, scored very high in the localization and classification of the ILSVRC-2014, so that's why I chose it - more details on <a href = \"https://www.robots.ox.ac.uk/~vgg/research/very_deep/\" target = \"_blank\">this conference paper</a>. There's actually a lot of VGG-16 pre-trained implementations in different languages, but I decided to break it down and code it myself, since I did this projects to learn more about convolutional neural networks.</p>\n<hr>\n<h2>Classification results</h2>\n<p>   The following graph shows the validation accuracies for the base model, its incrementation with dropout layers and its version with dropout and data augmentation. It is clear that the model using dropout layers and data augmentation technoiques performs better that the other two, after 40 training epochs. In fact, with very little time spent tuning the model, it reaches a validation accuracy of about 70% on a dataset with two quasi-identical cheeses.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 432px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ef1425fa33f9fc68c84d89e335b0d0ca/0e0c3/cheezam-cnn4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAACtUlEQVQ4y6WTzW7bRhDHN3XbpIceit6j9FFy7QM1BZo6aZvkDfoCTpTAb9CDesjBFwkGaIulGX6ZVEiK+qIkrrlccj/+xbKqnUtPHWCwg92ZH/47s0sAkK7ryNnZGTExIeRrQshXhJDPCSFfEELuE0KOCCFfEkIeHOKjQ445e2Dq3v/5noz+GJF/Ifcmk8n3eZ6/tm373dXV1Vvf919/8D68cV13GEfxieM6b0IvHHq+d2JdWSeO47yzbGs4mU7eztLZ747tfOM6bg88MtCiKH7Gf1gtaxRNAa45dt0OZVti05b9WskKlFGkcfpdlmQ98J4Bep73kylWSrVKaam0kkopWfFKLuul3PG9LG6Kfg+AlEqYVZgSznnrB8HDMAzvFGZZ9kRrbZgmUWutEO+vkdH0oFPfKpZKgbUSO9bpLeuwp7VMZ/EgmyU98DMDDILgqUnupJTFjuu43CCt9ihZi30jUXHVw5a0RbhmoK2G6LpeAmu4jMJgcB19qjDPfzAF+0bKgnLtFil25QbbRYp1WfaeL9coNltU+y3QVsA20WAlmprKMJ4NoiS9U3gdBU+5UFhtNrJc2ZqXEdBRQDZAvYLmFPKmBJoS2MXQ2xm0khpaobmpZJikgyj5eAfMsvRJ1WjQxV+yXvtatS1aqSCE/KR7gPpnIpBSQCptDDVrZBgEg+hw5R4YRvGPdJtjW33kTAhBay7i+FokSSIopYI3jWCMia5txWG6xjvDbxrW2rb90HGcux7m88UzVoZoOwrzMhhbIAh8RFEE13XheR6m0ynm8znW6zVWqxUWiwWWy2Xv8/n8UVEUtz+FjEajx/PZ7FWRF8dpmh47nvfC9/3nFxcXL8MwPLYs65Xnec+m0+nLy8vL5+Px+MX5+fkvlmX9Oh6PfxsOh9+enp6SWzPg/+Pm+xrQ3xQAP1rtjMlwAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"&quot;Testing accuracies&quot;\"\n        title=\"Figure - Validation accuracies\"\n        src=\"/static/ef1425fa33f9fc68c84d89e335b0d0ca/0e0c3/cheezam-cnn4.png\"\n        srcset=\"/static/ef1425fa33f9fc68c84d89e335b0d0ca/5a46d/cheezam-cnn4.png 300w,\n/static/ef1425fa33f9fc68c84d89e335b0d0ca/0e0c3/cheezam-cnn4.png 432w\"\n        sizes=\"(max-width: 432px) 100vw, 432px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>   I purposely chose not to include the VGG-16 results in this articles, as the current tuning of the model as can be seen on GitHub is not ideal. In fact, it barely reaches 60% validation accuracy on the state of things (though it reached above 80% when I submitted my project for grading) as I kept messing with the activation functions, dropout layers, batch normalization layers and so on and lost my working local copy. Stay tuned on GitHub for more recent updates!</p>\n<hr>\n<h2>Future work</h2>\n<h3>More cheeses</h3>\n<p>   The idea is to be able to classify most cheeses from pictures - and there are a LOT of them. But this requires me to build a more complete dataset, and in the state of things, will be difficult. Not only the number of images I can get from Google Images is limited to a few hundreds per cheese type, but some cheeses may be underdocumented. Also, the neural network will need to be tweaked a little to accomodate the higher number of labels.</p>\n<h3>An app?</h3>\n<p>   The end goal for this project is to build an actual application to be used on phones, to classify cheeses on the fly (<em>i.e</em> when you are at a restaurant and want to know what kind of cheese is in your platter). This is 100% doable, but an issue to keep in mind is that <u>cheeses on cheese platters are often cut in small slices</u> - which means I probaly should use a dataset of sliced cheeses to train my classifier. However, this might be more challenging than classifying whole cheeses or bigger slices. In the end, all hinges on choosing the proper way to build the dataset.</p>\n<hr>\n<p>Find this project on <a href = \"https://github.com/niwya/cheezam\">GitHub</a>.</p>\n</div>","categories":["Machine Learning","Python"],"date":"April 26, 2021","description":"A cheese classifier.","id":"c18de7be-f57d-5995-b354-9e95d83204e0","keywords":["Classifier","Cheeze","Image Scraping","Data Augmentation","Python","VGG-16"],"slug":"/projects/cheezam/","title":"Cheezam","readingTime":{"text":"7 min read"}},{"banner":{"alt":"Screenshot from the simulated restaurant","caption":"","src":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='660'%20height='400'%20viewBox='0%200%20660%20400'%20preserveAspectRatio='none'%3e%3cpath%20d='M183%20200v200l1-193V15h476V0H183v200M30%2053l1%202v26c-2%201-27%202-27%200l-2-1c-1%200-2%201-2%203l2%202%202-1a83%2083%200%200130%201c6-1%206%200%206%2025l-1%2023c-2%202-35%202-36%200-2-2-3%200-3%203%200%202%202%203%204%201%200-2%2033-1%2035%200a211%20211%200%20010%2050v10c-1%209-1%2010-3%207l-2-2v3c2%202%202%202-5%201h-5c0%201%2014%203%2016%202s2-1%201-2%200-16%201-18a129%20129%200%200139%200c2-1%2047-2%2049%200%201%202%204%200%204-2s-2-4-4-2a217%20217%200%2001-52%200c-1%202-34%201-36%200-2-2-2-44%200-47%202-1%2036-2%2036%200l2%201%203-1h47c1%202%204%200%204-2s-3-3-4-2a253%20253%200%2001-51%200c-3%202-35%201-37%200-2-2-2-45%201-47%202-1%2035-2%2035%200l2%201%203-1%205-1%205%201%202%201%203-1h32c1%202%204%200%204-2s-3-3-4-1-31%201-33%200V57c2-2%200-5-2-4s-3%205-1%205l1%209c0%208%200%209-2%209v7c0%201-8%202-11%200h-4a124%20124%200%2001-38-1c-6%201-7-1-7-14%200-9%200-11%202-13v-3c-2-1-4-1-5%201m320%202v5l3%2018c7%2024%209%2035%209%2037s-4%202%2059-1l32-2h2l-2-2-2-3h3c7-2%2010-1%2016%203l6%204-59%204-66%204v4c3%205%208%2027%208%2028-3%202-5-4-17-45-11-39-12-40-14-40l-4%201c-3%200-2%201%200%2010a23747%2023747%200%200123%2075c-1%202%200%209%202%2012l-3%202c-4%202-6%205-6%2011l-1%208%201%208c2%208%202%208%205%207l8-2%2026-3c2%200%202-1%202-7-1-13-6-22-13-24-12-2-5-3%2036-6h8l1%205c0%205%200%205%201%203l1-5v-1l5-1c3%200%203%200%204%204s3%206%202%201c-2-8-1-8%2016-9l8-1-1%203%201%202%201-3%201-1v5c0%203%200%204%201%203l1-4c0-3%200-3%203-3s4%201%202%205-2%2017-1%2021%201%204%206%203l11-1a229%20229%200%200119-3c1%200-1-6-4-10-2-2-2-4-1-7%200-5-1-10-5-10-2%200-2%200-1-6%200-6-1-8-5-8s-7%203-7%206c0%202%200%203-1%202l-1-1%201-10%202-10c1-1%2023-2%2024-1l-1%204c-1%204-1%207%201%203%201-5%201-5%202-3%200%202%201%202%201%201h4c3%200%203%201%203%204-1%204%201%204%202%200%200-4%203-8%203-4v14l-1%204%202-4c1-3%202-4%203-3l5%201c4%200%205%200%204%204%200%204%201%203%202-1%201-3%204-5%204-2s2%201%202-3c2-8%206-13%206-7%200%202%200%202%201%201%201-4%201-6-1-5l-1-2c1-1%200-2-1-2l-12-1c-12-2-10-4%205-5%209%200%2010%200%2012%202%201%202%202%202%204%200%202-1%203-1%203%201l-1%202c-1%200-4%206-6%2013-1%207-1%208%201%204l1-3h6l7%201v7l1-1a66%2066%200%200112-21h5c6%201%207%201%205%204v3l6-12%203-1%202-1c0-3%2011-3%2017-2%206%202%208%204%203%202-6-1-15%200-16%201l-3%201-2%202%201%201v2l1%201-2%205v3c2-2%204%200%202%203s-1%204%202%200c2-3%203-3%2010-1h2c2%200%202%201%201%203v4l1-1c0-2%206-10%207-9l2-1%202-2v3c-3%204-2%206%200%202%202-3%203-3%205-3%205%200%207%201%207%203l1%201%201%202v1l2-2%202-3%2020%206c0%201-3%2010-5%2012l-1%203-3-2c-11-8-26-8-25-1v1c-2-1-2%200-2%204-1%206-2%207-8%201-7-6-11-7-18-7-8%200-10%202-8%206%201%204%201%205-1%201-2-2-2-2-2%204l-3%2011c-2%204-3%207-1%204%201-1%202-1%205%202s4%203%203%205c-2%202%200%203%202%201%203-4%203%200-1%206l-2%207%205-5%206-8c2%200%2015-2%2016-1%201%200-5%208-8%2010l-5%207c-2%206-2%206%200%208%201%202%201%202-1%202l-3%201-4%202c-3%202-4%202-8%200l-4-1c-2%201-2%205-1%205l1%201%201%201c1%201%201%201-1%204l-2%203-8-4c-7-4-7-4-10-3l-2%204c0%201-2%201-7-1-4-2-4-1-1-6%202-3%202-3%2022-6l21-4c2-2%200-3-4-2a373%20373%200%2001-23%204c-15%202-14%202-15%200l-6-5c-5-3-6-3-9-2-4%202-7%205-8%2010l-1%203-41%207c-10%201-8%203-19-19-8-16-10-18-8-11v7c-1%205%200%207%201%203%202-7%204%203%202%2012-1%204%200%206%201%205l1-5%201-8c1-4%203%203%203%2011%200%206%202%2014%203%2011l6-1%206-2c2%200%202%201%201%206l-5%201c-4%200-12%203-8%203l2%201c-2%201%202%203%207%203h5l-6%201-6%201h3c2%200%202%200%200%201l-1%201c2%200%204%203%204%204l2%205%202%206-1%205c0%204%203%204%204%200l1-4c2%200%205%203%207%207l8%208%204%204-3%207c-5%2013-2%2015%204%203l3-8h15c6-1%206-1%205%204-2%207%200%207%203%200%204-11%206-15%2014-19%207-3%2014-8%209-6-10%204-26%207-31%205-4-1-4-1%204-1%205%200%208%200%2010-2h2l4-3%206-5c2-1%203-5%201-7v-1c1-1%207%207%207%2010-1%202%200%203%205%206l4%204c-5%206-10%2014-10%2016l3%202%206-8%206-9h8c4%200%207%201%207%202a805%20805%200%2001-3%209l2-1c1-2%208-13%2011-15%204-4%2010-7%2017-10%203-1%207-2%209-4l1-1c-9%203-24%204-22%202h1l1-1c-1-1-1-1%200%200l1-1c-1-1-1-1%200%200l1-1h1l1-1v-1l3-1%204-2c2%201%202%200%201-2%200-3%201-4%205-9l7-5v-1l-1-2c0-2%200-2%201-1l2%201v-1l-1-2-2-2c-2%201-4-1-4-2l1-1c1%200%204-2%206-5%202-4%204-6%205-5l1-1c-1-1%204-9%206-9l5%203%207%202%204-1%201-2-2-1c-2%200-17-7-43-21-10-5-10-5%206-7l11-2h1l13-1c11-1%2011-2%209-3-1-2-1-2%202-2l3%201v-12l-1-12c-2%201-2%200-2-1%200-2%202-4%202-2%201%201%201-3%201-9%200-11%200-11-2-11a4833%204833%200%2000-45-11l16-35c2-2%202-2%2016-2h15V51H349l1%204m-67%2036l-6%203%207%2015a515%20515%200%200111%2025%203641%203641%200%200127%2054l6-1%203-1-37-90c-4-8-3-8-11-5M70%20110v9h19v-19H70v10m52%200v9h19v-19h-19v10m-52%2050v9h19v-19H70v10m52%200v9h19v-19h-19v10m384%203c-3%201-4%204-3%208%202%207%202%208%200%205l-2-4v5c0%205%201%206%203%206l2-1%208-1%2016-1%208-1-3-4c-7-11-11-13-20-13l-9%201m90%2012l2%202v8h5c7-1%2013-3%2012-4l-19-9c-2%200-2%200%200%203M70%20203v9h19v-19H70v10m52%200v9h19v-19h-19v10m309-5c-12%202-17%203-17%201h-3l-2%203c-4%201-3%2011%201%2011%202%200%202-1%202-4-1-9%202-8%203%201%200%205%201%205%203%205l3%202v1l-1%202c-1%201-1%202-2%201-2%200%200%208%202%2013v5c0%201%202%204%204%204v-5c2%200%203%201%203%202l3%204c3%204%203%206-2%208l-4%203h2c1-2%204-2%206%200h2l4-1c4%200%206-1%203-2h-4l-2-3c-2-3-5-11-4-12%203-2%201-12-2-12-2%200-2%200-1-1%203-1%202-4%200-4-4%200%200-6%205-7%204%200%204%200%204%202s0%202%202%200v-3l-2-1%202-1%201-3-4-10-5%201m206%207l6%204%207%204c1%200-3%206-15%2020-4%204-4%206-2%206l14-14c0-1%201-1%206%202%204%202%205%204%204%204-1%202%200%203%202%203l1-11v-12h-3l-7-3c-4-2-13-5-13-3m-117%2018l-15%202c-12%202-12%202-12%205-1%203%200%203%205%206l6%204%201-3c1-4%204-7%206-5%203%202%208%200%208-3l-1-2-2-1%202-1c1%200%202%200%201%201h1l5-1c1%201%202%201%203-1s-1-3-8-1m-203%2014c-4%202-12%2012-12%2015s4%208%207%2010%203%203-3%205c-5%202-8%206-10%2013-1%206-1%206%202%209%205%207%208%208%2012%208%206%200%2012%204%2014%209%200%202%200%202%205%202%2019-3%2038-14%2040-23l1-4c5-8%205-13%202-22-2-4-2-5-1-5v-1c-2%200-2%200-2-2%201-1%200-1-2-1h-3c-1-2-1-3%208-3h4l-2%201h2l4-1h2l2-1-2-1c-2-1-2-1%201-1%202%200%203%200%202-2l-1-2-36%206c-17%202-16%202-18-2l-3-5c-3-2-11-3-13-2m25%2014c-4%201-5%202-5%203l1%202%201-1v-1h3l-1%201-1%202c0%202%201%202%203%202l16%202c2%201%205-3%203-6l-2-2-1-1c1-2-9-2-17-1m36%2041l-2%205v-5l-1-4-1%201c0%201-5%206-11%2010-9%206-27%2011-43%2013-7%200-14-2-15-6-1-2-3-3-3-1%200%206%209%2011%2019%2011%207%200%208%200%2015%2012%204%208%206%209%207%207%201-1%201-2-1-7-4-6-4-6-1-5%204%200%2016-3%2022-6%206-4%207-4%2012%204%203%207%204%207%205%205%202-1%202-1-5-13-2-3-2-3%201-9%203-5%205-12%204-16l-2%204m277%200c-6%203-9%206-8%2014%200%2010%202%2013%208%2013l5%203v-29c0-3%200-4-5-1m-15%2018l-2%204c0%204-3%2013-7%2020l-1%203%208%2013a4771%204771%200%200122%2033v-42l-12-19c-3-8-8-14-8-12m-61%201c-15%203-23%2012-22%2026%201%2012%201%2013%205%2010%207-6%2019-6%2029-2%207%204%2018%2016%2031%2033l14%2018%202%203h22v-5c0-5-4-13-26-45-22-34-34-42-55-38m-28%2034l-2%2013-2%204-1-4c0-4%200-4-3-1l-6%204c-2%201-4%202-3%203l-2%202v2c2%202%205%201%2010-3l4-2%202%207%206%2018%204%2011h8c7%200%208%200%207-1a288%20288%200%2001-21-63l-1%2010m-226%2033c-15%203-28%209-38%2018l-4%203h79v-3c-4-15-18-22-37-18'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/c1ba25fa133345e395b9e6274d279683/bc51f/autonomousWaiter.png","srcSet":"/static/c1ba25fa133345e395b9e6274d279683/41200/autonomousWaiter.png 165w,\n/static/c1ba25fa133345e395b9e6274d279683/f979a/autonomousWaiter.png 330w,\n/static/c1ba25fa133345e395b9e6274d279683/bc51f/autonomousWaiter.png 660w","sizes":"(min-width: 660px) 660px, 100vw"},"sources":[{"srcSet":"/static/c1ba25fa133345e395b9e6274d279683/322ad/autonomousWaiter.webp 165w,\n/static/c1ba25fa133345e395b9e6274d279683/de3b3/autonomousWaiter.webp 330w,\n/static/c1ba25fa133345e395b9e6274d279683/2b2b5/autonomousWaiter.webp 660w","type":"image/webp","sizes":"(min-width: 660px) 660px, 100vw"}]},"width":660,"height":400}}}},"body":"<!--**bold**\n    *italics*\n    ## headline\n    ### subheadline\n    #### subsubheadline -->\n<div style=\"text-align: justify\">\n<h2>I - The main idea</h2>\n<p>   Having a fully autonomous waiter in a restaurant, that can plan tasks on its own taking into account user inputs made via hand signs, is the main idea, basically. The main challenges in the realization of this goal are:</p>\n<ul>\n<li>How to make the agent capable of planning tasks on its own? How to represent the essential knowledge for this to be possible? How to make it so that new knowledge can be ingested?</li>\n<li>How to recognize hand signs? How to translate hand signs to goals?</li>\n</ul>\n<p>   In this project, as a two-people team, we designed a knowledge domain in <a href = \"https://arxiv.org/abs/1301.1386\" target = \"_blank\">CR-Prolog SPARC</a>, an Anser Set Progamming (ASP) language with an inference module, that can communicate with a simulated environment made with PyBullet in which a user can interact with an autonomous waiter in a restaurant, using hand signs to let it know what is expected of it. The goal is to have the autonomous agent capable of serving clients on its own, but also to provide support to other employees when required. The user can theoretically be an employee of the restaurant, or a customer - each category of people has its own designated hand signs, and making the distinction bewteen the two has to be made using computer vision. This recognition feature is not implemented, but one way to do it would be looking for an employee badge in the video feed of the autonomous agent.</p>\n<p>   The current state of our project is a simulation in which the user can give orders to the agent by using hand signs via an external camera, as well as add new hand signs. As of now, there is no way to translate newly learned hand signs to actual knowledge in the knowledge domain other than manually inputting the logic in the code.</p>\n<hr>\n<h2>II - The knowledge domain</h2>\n<p>   I've summed up all there is to know about the technical side of my work in this project in <a href = \"/aspSparc.pdf\" target = \"_blank\">this report</a>. To put it in simpler words - I coded a knowledge domain using an improved version of CR-Prolog (SPARC - this version essentially adds consistensy restoring rules to the base language, as well as sorts, which can be thoughts of as types). This knowledge domain contains the rules of the world my autonomous agent acts upon - which is, in this case, a restaurant. Among those rules are actions that can be performed, and a list of restrictions. Those actions are triggered by observations made, in this case, in the simulated domain, but ultimately by hand sign made by users in the real world.</p>\n<p><em>Quick note: I'll be talking about \"the ASP\" later - this refers to the file in which the knowledge domain is coded in CR-Prolog SPARC, not to ASP in general. I may also use this shortcut a lot in the report and GitHub repositories mentioned here.</em></p>\n<h3>The restaurant</h3>\n<p>   The restaurant layout that you can see in the simulation is hardcoded in the ASP. How things are is the restaurant is divided into nodes where the agent can be at. Those nodes are conveniently placed in the room - by tables, at the entrance or at the counter, for instance. This might be very unpractical in real life applications, but was the simplest approach to code the environment so that the logic in the ASP stays simple and manageable for me to code and test. In the ASP file, the nodes are actually directly associated to objects and places, to enforce the fact that the agent has to be at a particular place to perform an action (as an example, the agent could not give the bill to a certain table if it was standing on a node at the opposite end of the restaurant).</p>\n<h3>The actions</h3>\n<p>   The agent can perform 4 actions, though more were initially thought of. Those actions are:</p>\n<ul>\n<li>Move around node by node in the restaurant, if that node is free</li>\n<li>Pick up and seat a party of customers (that accounts for 2)</li>\n<li>Bring the bill to a table</li>\n</ul>\n<p>   This is not enough for an autonomous agent to actually be able to serve in a restaurant. A more thorough knowledge domain would be required - and unit testing of each added set of rules (to make sure adding rules does not break anything - I've seen some weird things happening sometimes). The base ASP file featured in my GitHub (see below) can totally be built upon to add more rules - but they still have to be hardcoded, support for them needs to be added in the simulation, and a triggering hand sign needs to be associated to them.</p>\n<h3>The planning</h3>\n<p>   The ASP needs to know how many steps it should plan on - this is also hardcoded. There is no native minimal planning module - meaning that if a goal can be reached in 4 steps but we give 8 steps as input, the plan to reach the goal will be 8 steps long - so I also added one manually. How it works right now is the ASP starts at 1 step and is updated until it can come up with a valid plan. However, this approach does not guarantee the existence of a plan. In the current state of things, the number of steps to plan on is incremented until a \"safe\" number of steps has passed to conclude there is no plan - this however, is computationally heavy and not a viable solution for an embedded use.</p>\n<h3>Want to know more?</h3>\n<p>   You can find the knowledge domain designed for this project on <a href = 'https://github.com/niwya/restaurant_ASP_SPARC' target=\"_blank\"> my GitHub </a>, as well as a more in-depth description of the knowledge domain I coded on <a href = 'https://github.com/ArthurFDLR/Commonsense-Reasoning-Bot' target=\"_blank\"> Arthur's GitHub </a> (most of the READ ME is about the ASP code). There is also a flowchart explaining how the ASP communicates with the simulated world.</p>\n<hr>\n<h2>III - The simulation</h2>\n<p>   You can find the simulated environment designed for this project - and how to make it work - on <a href = 'https://github.com/ArthurFDLR/Commonsense-Reasoning-Bot' target=\"_blank\"> Arthur's GitHub </a> (at the beginning of the READ ME). I only worked on how to communicate the outputs of the ASP to the simulation and to update the ASP directly - you can find the details <a href = \"https://github.com/ArthurFDLR/Commonsense-Reasoning-Bot/blob/master/commonsense_reasoning_bot/ASP/CommunicationASP.py\" target = \"_blank\"> there </a>.</p>\n<hr>\n<h2>IV. Going further...</h2>\n<p>   One of the initial goals for this project was to add the ability for the agent to function on an incomplete knowledge domain, learning new rules on the fly, without having to hardcode them manually. In the current case, when confronted with an unknown situation, the agent does nothing - it cannot ingest new knowledge.</p>\n<hr>\n<h2>Credits &#x26; Publications</h2>\n<p>   This project was undertaken under the supervision of <a href =\"https://www.cs.bham.ac.uk/~sridharm/\" target = \"_blank\"> Dr. Mohan Sridharan </a> at the School of Computer Science in Birmingham, UK, and with the help of <a href = \"https://arthurfindelair.com/\" target = \"_blank\"> Arthur Findelair </a>, at that time student at ISAE-ENSMA, who was in charge of running the simulation.</p>\n<p>   A publication of an updated version of this project will be showcased at the NMR 2022 conference in Haifa, Israel. Many thanks to Kévin Gloaguen, student at ISAE-ENSMA, for taking over this project and adding functionalities.</p>\n</div>","categories":["Internship","Research"],"date":"September 01, 2020","description":"Basic restaurant knowledge domain in CR-Prolog SPARC for an autonomous agent to plan diverse  customer-oriented tasks and collaborated on linking it to a simulated environment made with PyBullet.","id":"e3291baa-c53d-5884-9f40-45debfd1ffe7","keywords":["Knowledge Representation","Answer Set Programming","CR-Prolog","SPARC","Research"],"slug":"/projects/autonomous-waiter/","title":"Autonomous Robot Waiter","readingTime":{"text":"7 min read"}}]}},"staticQueryHashes":["1094019748","36698639"]}